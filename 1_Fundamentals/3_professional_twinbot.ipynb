{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c637bbd5",
   "metadata": {},
   "source": [
    "# Twin Professional Chatbot\n",
    "\n",
    "Professional Twin chatbot using the Ollama LLM framework. The chatbot is designed to provide information about my professional background based on a summary text file and a linked PDF profile and portfolio website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77284245",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "325193d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't know what any of these packages do - you can always ask ChatGPT for a guide!\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pypdf import PdfReader\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a453cca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1e25be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is set:\n",
      "\t- OLLAMA_BASE_URL = http://localhost:11434/v1\n",
      "\t- OLLAMA_MODEL_LLAMA = llama3.2\n",
      "\t- OLLAMA_MODEL_PHI = phi4-mini\n"
     ]
    }
   ],
   "source": [
    "OLLAMA_API_KEY = os.getenv('OLLAMA_API_KEY')\n",
    "OLLAMA_BASE_URL = os.getenv('OLLAMA_BASE_URL')\n",
    "OLLAMA_MODEL_LLAMA = os.getenv('OLLAMA_MODEL_LLAMA')\n",
    "OLLAMA_MODEL_PHI = os.getenv('OLLAMA_MODEL_PHI')\n",
    "\n",
    "# Check Ollama\n",
    "if OLLAMA_API_KEY and OLLAMA_BASE_URL and OLLAMA_MODEL_LLAMA and OLLAMA_MODEL_PHI:\n",
    "    print(f\"Ollama is set:\")\n",
    "    print(f\"\\t- OLLAMA_BASE_URL = {OLLAMA_BASE_URL}\")\n",
    "    print(f\"\\t- OLLAMA_MODEL_LLAMA = {OLLAMA_MODEL_LLAMA}\")\n",
    "    print(f\"\\t- OLLAMA_MODEL_PHI = {OLLAMA_MODEL_PHI}\")\n",
    "else:\n",
    "    print(\"Ollama parameter(s) not set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55e5ed0",
   "metadata": {},
   "source": [
    "Install additional libraries for web scraping and data handling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15789c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nc/Library/CloudStorage/OneDrive-Personal/github/agents/.venv/bin/python: No module named pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests beautifulsoup4 lxml pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f663f35",
   "metadata": {},
   "source": [
    "## Load my professional data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8bd89c",
   "metadata": {},
   "source": [
    "Summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5d77884",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"me/summary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    summary = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ffab04",
   "metadata": {},
   "source": [
    "LinkedIn profile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8fa9b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(\"me/linkedin.pdf\")\n",
    "linkedin = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        linkedin += text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab7124d",
   "metadata": {},
   "source": [
    "Website portfolio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2a56e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from typing import Dict, List, Optional, Any, Iterator\n",
    "import re\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ad7a0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://cordovank.github.io\"\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (compatible; your-email@example.com)\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a635b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_html(url: str, timeout: int = 10) -> str:\n",
    "    resp = requests.get(url, headers=HEADERS, timeout=timeout)\n",
    "    resp.raise_for_status()\n",
    "    return resp.text\n",
    "\n",
    "def make_soup(html: str):\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    # remove scripts/styles which pollute text\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\", \"template\"]):\n",
    "        tag.decompose()\n",
    "    return soup\n",
    "\n",
    "def get_meta(soup: BeautifulSoup) -> Dict[str, Optional[str]]:\n",
    "    title = soup.title.string.strip() if soup.title and soup.title.string else None\n",
    "    meta_desc = (soup.find(\"meta\", {\"name\":\"description\"}) or {}).get(\"content\")\n",
    "    canonical = (soup.find(\"link\", {\"rel\":\"canonical\"}) or {}).get(\"href\")\n",
    "    return {\"title\": title, \"meta_description\": meta_desc, \"canonical\": canonical}\n",
    "\n",
    "def normalize_links(soup: BeautifulSoup, base_url: str) -> List[str]:\n",
    "    raw = [a.get(\"href\") for a in soup.select(\"a[href]\")]\n",
    "    normalized = []\n",
    "    for h in raw:\n",
    "        if not h:\n",
    "            continue\n",
    "        absurl = urljoin(base_url, h)\n",
    "        # optionally skip mailto: or javascript:\n",
    "        if absurl.startswith(\"javascript:\"):\n",
    "            continue\n",
    "        normalized.append(absurl)\n",
    "    # unique preserve order\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for u in normalized:\n",
    "        if u not in seen:\n",
    "            seen.add(u)\n",
    "            out.append(u)\n",
    "    return out\n",
    "\n",
    "def get_visible_text(soup: BeautifulSoup, max_chars: int = 100000) -> str:\n",
    "    # prefer main/article sections if present\n",
    "    selectors = [\"main\", \"article\", \"body\"]\n",
    "    for sel in selectors:\n",
    "        node = soup.select_one(sel)\n",
    "        if node:\n",
    "            text = node.get_text(separator=\"\\n\").strip()\n",
    "            if text:\n",
    "                return text[:max_chars]\n",
    "    # fallback\n",
    "    return soup.get_text(separator=\"\\n\").strip()[:max_chars]\n",
    "\n",
    "def extract_section_by_id(soup: BeautifulSoup, id_name: str) -> Dict:\n",
    "    node = soup.find(id=id_name)\n",
    "    if not node:\n",
    "        return {\"id\": id_name, \"present\": False}\n",
    "    # title: first header\n",
    "    header = None\n",
    "    for htag in [\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"]:\n",
    "        h = node.find(htag)\n",
    "        if h and h.get_text(strip=True):\n",
    "            header = h.get_text(strip=True)\n",
    "            break\n",
    "    text = node.get_text(separator=\"\\n\", strip=True)\n",
    "    links = [urljoin(\"\", a.get(\"href\")) for a in node.select(\"a[href]\")]\n",
    "    return {\"id\": id_name, \"present\": True, \"header\": header, \"text\": text, \"links\": links}\n",
    "\n",
    "def extract_portfolio_items(soup: BeautifulSoup, base_url: str) -> List[Dict]:\n",
    "    items = []\n",
    "    for item in soup.select(\".portfolio-item\"):\n",
    "        title_node = item.select_one(\".portfolio-txt h4\") or item.select_one(\"h4\")\n",
    "        title = title_node.get_text(strip=True) if title_node else None\n",
    "        tags = [t.get_text(strip=True) for t in item.select(\".portfolio-tags span\")]\n",
    "        img = item.select_one(\"img\")\n",
    "        img_src = urljoin(base_url, img.get(\"src\")) if img and img.get(\"src\") else None\n",
    "        link = None\n",
    "        # try to find repo/external link inside overlay or links\n",
    "        a = item.select_one(\"a[href]\")\n",
    "        if a:\n",
    "            link = urljoin(base_url, a.get(\"href\"))\n",
    "        items.append({\"title\": title, \"tags\": tags, \"img\": img_src, \"link\": link})\n",
    "    return items\n",
    "\n",
    "_email_re = re.compile(r\"[a-zA-Z0-9.\\-+_]+@[a-zA-Z0-9.\\-+_]+\\.[a-zA-Z]+\")\n",
    "_phone_re = re.compile(r\"(?:\\+?\\d{1,3}[-.\\s]?)?(?:\\(\\d{2,4}\\)|\\d{2,4})[-.\\s]?\\d{3}[-.\\s]?\\d{3,4}\")\n",
    "\n",
    "def extract_contact_info(soup: BeautifulSoup) -> Dict:\n",
    "    text = soup.get_text(\" \", strip=True)\n",
    "    emails = list(set(_email_re.findall(text)))\n",
    "    phones = list(set(_phone_re.findall(text)))\n",
    "    social = {}\n",
    "    # find common social links\n",
    "    for a in soup.select(\"a[href]\"):\n",
    "        href = a.get(\"href\")\n",
    "        if \"linkedin.com\" in href:\n",
    "            social.setdefault(\"linkedin\", []).append(href)\n",
    "        if \"github.com\" in href:\n",
    "            social.setdefault(\"github\", []).append(href)\n",
    "        if href.startswith(\"mailto:\"):\n",
    "            social.setdefault(\"mailto\", []).append(href)\n",
    "    # dedupe\n",
    "    for k in social:\n",
    "        social[k] = list(dict.fromkeys(social[k]))\n",
    "    return {\"emails\": emails, \"phones\": phones, \"social\": social}\n",
    "\n",
    "def extract_page(url: str) -> Dict:\n",
    "    html = fetch_html(url)\n",
    "    soup = make_soup(html)\n",
    "    meta = get_meta(soup)\n",
    "    links = normalize_links(soup, url)\n",
    "    visible_text = get_visible_text(soup)\n",
    "    # sections present on your portfolio\n",
    "    sections = {sid: extract_section_by_id(soup, sid) for sid in [\"hero\",\"about\",\"resume\",\"portfolio\",\"contact\"]}\n",
    "    portfolio = extract_portfolio_items(soup, url)\n",
    "    contact = extract_contact_info(soup)\n",
    "    return {\n",
    "        \"url\": url,\n",
    "        \"meta\": meta,\n",
    "        \"links\": links,\n",
    "        \"visible_text\": visible_text,\n",
    "        \"sections\": sections,\n",
    "        \"portfolio_items\": portfolio,\n",
    "        \"contact\": contact\n",
    "    }\n",
    "\n",
    "html = fetch_html(URL)\n",
    "soup = make_soup(html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2efaf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# data = extract_page(\"https://cordovank.github.io\")\n",
    "# print(json.dumps({k:v for k,v in data.items() if k!='visible_text'}, indent=2))\n",
    "# Optionally save:\n",
    "# with open(\"cordovank_page.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "255165ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = extract_page(\"https://cordovank.github.io\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "561b6d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Nellie's Personal Portfolio\n",
      "Contact emails: ['cordova.nellie@outlook.com']\n",
      "First portfolio item: {'title': 'RAG System with Guardrails', 'tags': ['FastAPI', 'LLM', 'FAISS'], 'img': 'https://cordovank.github.io/assets/img/project/rag.webp', 'link': 'https://cordovank.github.io/assets/pages/project-details/rag.html'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Title:\", data[\"meta\"][\"title\"])\n",
    "print(\"Contact emails:\", data[\"contact\"][\"emails\"])\n",
    "print(\"First portfolio item:\", data[\"portfolio_items\"][0] if data[\"portfolio_items\"] else \"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e37bbaf",
   "metadata": {},
   "source": [
    "Save to JSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f78519b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- path helpers ----------------------------------------------------------\n",
    "OUTPUT_DIR = \"me/\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "PAGE_JSON_PATH = os.path.join(OUTPUT_DIR, \"cordovank_page.json\")\n",
    "PAGE_JSONL_PATH = os.path.join(OUTPUT_DIR, \"cordovank_chunks.jsonl\")\n",
    "\n",
    "# --- save/load helpers ----------------------------------------------------\n",
    "def save_json(obj: Dict[str, Any], path: str = PAGE_JSON_PATH, indent: int = 2) -> None:\n",
    "    \"\"\"Save dict to JSON (UTF-8).\"\"\"\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=indent)\n",
    "    print(f\"Saved JSON -> {path}\")\n",
    "\n",
    "def load_json(path: str = PAGE_JSON_PATH) -> Dict[str, Any]:\n",
    "    \"\"\"Load JSON from disk and return as dict.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# --- very small schema / sanity check -------------------------------------\n",
    "# optional: lightweight validation you can expand or replace with jsonschema\n",
    "def basic_validate_page(obj: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"Return list of problems (empty list == ok).\"\"\"\n",
    "    problems = []\n",
    "    if not isinstance(obj, dict):\n",
    "        problems.append(\"Top-level object is not a dict.\")\n",
    "        return problems\n",
    "    for key in [\"url\", \"meta\", \"visible_text\"]:\n",
    "        if key not in obj:\n",
    "            problems.append(f\"Missing key: {key}\")\n",
    "    # meta should have at least title (or None)\n",
    "    if \"meta\" in obj and not isinstance(obj[\"meta\"], dict):\n",
    "        problems.append(\"meta is not a dict\")\n",
    "    return problems\n",
    "\n",
    "# --- chunking for embeddings / RAG ---------------------------------------\n",
    "def chunk_text(text: str, max_chars: int = 2000, overlap: int = 200) -> Iterator[str]:\n",
    "    \"\"\"Yield chunks of text for embeddings. Uses simple sliding window by characters.\n",
    "    For better behavior split by sentences/paragraphs (you can split by '\\n\\n' etc.).\"\"\"\n",
    "    if not text:\n",
    "        return\n",
    "    start = 0\n",
    "    text_len = len(text)\n",
    "    while start < text_len:\n",
    "        end = min(start + max_chars, text_len)\n",
    "        chunk = text[start:end]\n",
    "        yield chunk\n",
    "        if end == text_len:\n",
    "            break\n",
    "        start = max(0, end - overlap)\n",
    "\n",
    "def page_to_jsonl_chunks(page_obj: Dict[str, Any], out_path: str = PAGE_JSONL_PATH) -> int:\n",
    "    \"\"\"Write JSONL where each line is a chunk record with metadata for retrieval/embedding.\"\"\"\n",
    "    visible = page_obj.get(\"visible_text\", \"\")\n",
    "    url = page_obj.get(\"url\")\n",
    "    title = page_obj.get(\"meta\", {}).get(\"title\")\n",
    "    section_meta = page_obj.get(\"sections\", {})  # optional: you can create section-specific chunks too\n",
    "\n",
    "    count = 0\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for i, chunk in enumerate(chunk_text(visible, max_chars=1500, overlap=200)):\n",
    "            record = {\n",
    "                \"id\": f\"{os.path.basename(out_path)}::chunk::{i}\",\n",
    "                \"url\": url,\n",
    "                \"title\": title,\n",
    "                \"chunk_index\": i,\n",
    "                \"text\": chunk,\n",
    "                # minimal metadata to aid retrieval\n",
    "                \"meta\": {\n",
    "                    \"source\": \"portfolio_page\",\n",
    "                    \"section\": None,\n",
    "                }\n",
    "            }\n",
    "            fout.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "            count += 1\n",
    "    print(f\"Wrote {count} chunk(s) -> {out_path}\")\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb4b046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Extract page data\n",
    "# data = extract_page(\"https://cordovank.github.io\")\n",
    "\n",
    "# 2) Save full page JSON\n",
    "# save_json(data, PAGE_JSON_PATH)\n",
    "\n",
    "# 3) Load and validate\n",
    "# loaded = load_json(PAGE_JSON_PATH)\n",
    "# problems = basic_validate_page(loaded)\n",
    "# if problems:\n",
    "#     print(\"Validation problems:\", problems)\n",
    "# else:\n",
    "#     print(\"Loaded JSON ok:\", PAGE_JSON_PATH)\n",
    "\n",
    "# 4) Create chunked JSONL for embeddings\n",
    "# page_to_jsonl_chunks(loaded, PAGE_JSONL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1254e9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved JSON -> me/cordovank_page.json\n"
     ]
    }
   ],
   "source": [
    "save_json(data, PAGE_JSON_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7fb6c40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded JSON ok: me/cordovank_page.json\n"
     ]
    }
   ],
   "source": [
    "loaded = load_json(PAGE_JSON_PATH)\n",
    "problems = basic_validate_page(loaded)\n",
    "if problems:\n",
    "    print(\"Validation problems:\", problems)\n",
    "else:\n",
    "    print(\"Loaded JSON ok:\", PAGE_JSON_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc52a41",
   "metadata": {},
   "source": [
    "## Prepare Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13cd870",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key=OLLAMA_API_KEY)\n",
    "\n",
    "# using OLLAMA_MODEL_LLAMA and OLLAMA_MODEL_PHI\n",
    "model_name = OLLAMA_MODEL_LLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6ba4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Nellie Cordova\"\n",
    "\n",
    "system_prompt = f\"You are acting as {name}. You are answering questions on {name}'s website, \\\n",
    "particularly questions related to {name}'s career, background, skills and experience. \\\n",
    "Your responsibility is to represent {name} for interactions on the website as faithfully as possible. \\\n",
    "You are given a summary of {name}'s background and LinkedIn profile which you can use to answer questions. \\\n",
    "Be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "If you don't know the answer, say so.\"\n",
    "\n",
    "system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "system_prompt += f\"With this context, please chat with the user, always staying in character as {name}.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083075f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = ollama.chat.completions.create(model=model_name, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e55e0b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Launch the chatbot \n",
    "\n",
    "Run the next cell and ask your professional twin questions about you!\n",
    "\n",
    "Questions to try:\n",
    "- Tell me a bit about yourself.\n",
    "- What is your greatest accomplishment?\n",
    "- What would you say are your top skills?\n",
    "- What is a challenge that you encountered and needed to overcome?\n",
    "- What are you looking for in your next role?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bd42d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9260a85",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
